name: Benchmark Run

on:
  workflow_dispatch:
    inputs:
      tools:
        description: 'Comma-separated tool names (leave empty for all)'
        required: false
        type: string
      challenges:
        description: 'Comma-separated challenge IDs (leave empty for all)'
        required: false
        type: string

permissions:
  contents: write

jobs:
  run-benchmark:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package and dependencies
        run: |
          pip install -e ".[dev]"
          # Install tools needed for benchmarking
          pip install pr-agent xai-review
          npm install -g shippie

      - name: Setup tools
        run: crb setup

      - name: Run benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CRB_NUM_RUNS: 3
          CRB_TOOL_MODEL: ${{ secrets.CRB_TOOL_MODEL || 'gpt-4o' }}
        run: |
          # Prepare input parameters
          TOOLS_PARAM=""
          CHALLENGES_PARAM=""

          if [ -n "${{ github.event.inputs.tools }}" ]; then
            TOOLS_PARAM="--tools ${{ github.event.inputs.tools }}"
          fi

          if [ -n "${{ github.event.inputs.challenges }}" ]; then
            CHALLENGES_PARAM="--challenges ${{ github.event.inputs.challenges }}"
          fi

          # Run benchmark
          crb run $TOOLS_PARAM $CHALLENGES_PARAM --output-dir results/runs/$(date +%Y%m%d_%H%M%S)

      - name: Evaluate results
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          crb evaluate --run-dir results/latest

      - name: Generate reports
        run: |
          crb report --run-dir results/latest --format both

      - name: Process historical data
        run: |
          python scripts/process_historical.py

      - name: Update dashboard data
        run: |
          python scripts/update_dashboard.py

      - name: Commit results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add docs/data/
          git add results/
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          git commit -m "Benchmark results: $TIMESTAMP" || echo "No changes to commit"
          git push

  notify-failure:
    runs-on: ubuntu-latest
    needs: [run-benchmark]
    if: ${{ failure() }}
    steps:
      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const date = new Date().toISOString().split('T')[0];
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Benchmark run failed - ${date}`,
              body: `The automated benchmark run failed on ${date}.\n\nWorkflow run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['benchmark', 'ci-failure']
            });